[["topicmodeling.html", "Chapter 1 Topic Modeling 1.1 LDA 1.2 CTM 1.3 STM", " Chapter 1 Topic Modeling Topic models are unsupervised ML models that identify topics as clusters of words with an associated probability distribution, and a probability distribution of topics within each document. 1.1 LDA Latent Dirichlet allocation (LDA) is an instance of a general family of mixed membership models that decompose data into latent components. Latent refers to unidentified topics. Dirichlet refers to the type of distribution followed by the words in the the topics and by the topics in the documents. Algorithm LDA assumes each document is created by a generative process where topics are included according to probabilities and words are included in the topics according to probabilities. The LDA algorithm determines what those probabilities are. The algorithm is: For each document \\(d_i\\), randomly assign each word to one of the K topics. Note that each \\(w_j\\) may be assigned to a different topic in each documents. For each document, tabulate the number of words in each topic, a \\(d \\times K\\) matrix. For each word, tabulate the sum of occurrences across all documents, a \\(w \\times K\\) matrix. Resample a single instance of a word from the corpus and remove it from the analysis, decrementing the documents topic count and the words topic count. Calculate the gamma matrix, \\(\\gamma\\), and the beta matrix, \\(\\beta\\). the gamma matrix is the probability distribution of topics for each document, \\[p(t_k|d_i) = \\frac{n_{ik} + \\alpha}{N_i + K \\alpha}\\] were \\(n_{ik}\\) is the number of words in document \\(i\\) for topic \\(k\\), \\(N_i\\) is the total number of words in \\(i\\), and \\(\\alpha\\) is a hyperparameter. For each \\(d_i\\), \\(\\sum_{k \\in K} \\gamma_{ik} = 1\\). the beta matrix is the probability distribution of words for each topic, \\[p(w_j|t_k) = \\frac{m_{j,k} + \\beta}{\\sum_{j \\in V}m_{j,k} + V\\beta}\\] where \\(m_{j,k}\\) is the corpus-wide frequency count of word \\(w_j\\) to topic \\(k\\), \\(V\\) is the number of distinct words in the corpus, and \\(\\beta\\) is a hyperparameter. For each \\(t_k\\), \\(\\sum_{j \\in V} \\beta_{kj} = 1\\). Perform Gibbs sampling. Calculate the joint probability distribution of words for each document and topic, \\(p(w_j|t_k,d_i) = p(t_k|d_i)p(w_j|t_k)\\). Assign each word, \\(w_j\\), to the topic with the maximum joint probability. Repeat steps 3-6 for all of the words in all of the documents. Repeat steps 3-7 for a pre-determined number of iterations. LDA thus has 3 hyperparameters: document-topic density factor, \\(\\alpha\\), topic-word density factor, \\(\\beta\\), and topic count, \\(K\\). \\(\\alpha\\) controls the number of topics expected per document (large \\(\\alpha\\) = more topics). \\(\\beta\\) controls the distribution of words per topic (large \\(\\beta\\) = more words). Ideally, you want a few topics per document and a few words per topics, so, \\(\\alpha\\) and \\(\\beta\\) are typically set below one. \\(K\\) is set using a combination of domain knowledge, coherence, and exclusivity. Evaluation Held-out Likelihood (discussion of hold-out probability) (Wallach et al., 2009). Semantic Coherence Exclusivity Generally, the greater the number of topics in a model, the lower the quality of the smallest topics. One way around this is simply hiding the low-quality topics. The coherence measure (Mimno et al. 2011) evaluates topics. 1.2 CTM The Correlated Topic Model (CTM) (Blei and Lafferty 2007) builds on the LDA model (chapter 1.1). 1.3 STM Without the inclusion of covariates, STM reduces to a logistic-normal topic model, often called the Correlated Topic Model (CTM) (chapter 1.2). References "]]
