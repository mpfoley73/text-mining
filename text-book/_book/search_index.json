[["sentimentanalysis.html", "Chapter 2 Sentiment Analysis 2.1 Data Formats 2.2 Subjectivity Lexicons 2.3 Polarity Scoring 2.4 Word Counts 2.5 Comparison Cloud", " Chapter 2 Sentiment Analysis library(tidyverse) library(qdap) library(tidytext) library(magrittr) library(tm) library(ggthemes) Sentiment analysis is the extraction of the emotional intent of text. As much as possible, I will build examples using the text from The Adventures of Tom Sawyer, available on https://gutenberg.org and accessible using the gutenbergr package. library(gutenbergr) sawyer_raw &lt;- gutenberg_works(title == &quot;The Adventures of Tom Sawyer&quot;) %&gt;% gutenberg_download() skimr::skim(sawyer_raw) Table 2.1: Data summary Name sawyer_raw Number of rows 8832 Number of columns 2 _______________________ Column type frequency: character 1 numeric 1 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace text 0 1 0 78 2212 6579 0 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist gutenberg_id 0 1 74 0 74 74 74 74 74  sawyer_raw is a tibble with 8,832 rows, with one row per line of text and 0-78 characters per line. This is a corpus with a single document and no metadata (although you could get multiple books at once, and attach the title and author as metadata). Most text requires some cleaning. I will want to remove the title lines, and add add some metadata, including the chapter number and line number. sawyer &lt;- sawyer_raw %&gt;% tail(-455) %&gt;% # chapter 1 starts on line 456 mutate( is_chap = str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case = TRUE)), chapter = cumsum(is_chap) ) %&gt;% filter(text != &quot;&quot; &amp; !str_detect(text, regex(&quot;^chapter [\\\\divxlc]&quot;, ignore_case = TRUE))) %&gt;% mutate(line = row_number()) %&gt;% select(line, chapter, text) head(sawyer) ## # A tibble: 6 x 3 ## line chapter text ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 1 TOM! ## 2 2 1 No answer. ## 3 3 1 TOM! ## 4 4 1 No answer. ## 5 5 1 What&#39;s gone with that boy, I wonder? You TOM! ## 6 6 1 No answer. 2.1 Data Formats There are three very common text mining packages, each with their own data format. Whichever package you work in, there is a decent chance you will want to use a function from one of the others, so you need some fluency in them all. tm works with Corpus objects (raw text with document and corpus metadata). Many tm algorithms work with a document-term matrix (DTM), a sparse matrix with one row per document, one column per term, and values equaling the word count or tf-idf. quanteda works with Corpus objects (same as tm, but its own implementation). Many quanteda algorithms work with a document-feature matrix (DFM) (same as tms DTM, but its own version). tidytext works with tibbles. Many tidytext algorithms work with tidy data frames, a tibble with one row per token (usually a word, but possibly a large item of text), a frequency count column, and possibly other metadata columns. Lets take the sawyer_raw data frame and pre-process it for all three packages. tm Turn the character vector sawyer_raw$text into a text source with VectorSource(), then turn the text source into a corpus with vCorpus(). Clean the corpus with a series of utility functions. One particularly important function, removeWords(), removes stop words, plus any custom stop words. I would normally add tom because it is so ubiquitous throughout the text. However, in this case I wont because stopwords includes valence shifting words like very which are used in polarity scoring. I can remove them later for other exercises. (sawyer_tm &lt;- VCorpus(VectorSource(sawyer$text)) %&gt;% tm_map(content_transformer(replace_abbreviation)) %&gt;% tm_map(removePunctuation) %&gt;% tm_map(removeNumbers) %&gt;% tm_map(content_transformer(tolower)) %&gt;% tm_map(removeWords, c(stopwords(&quot;en&quot;), &quot;tom&quot;)) %&gt;% tm_map(stripWhitespace)) ## &lt;&lt;VCorpus&gt;&gt; ## Metadata: corpus specific: 0, document level (indexed): 0 ## Content: documents: 6349 Each document in the sawyer_tm VCorpus is a line of text. Use DocumentTermMaterix() to convert the vCorpus into tms bag-of-words format, DTM. (sawyer_tm_dtm &lt;- DocumentTermMatrix(sawyer_tm)) ## &lt;&lt;DocumentTermMatrix (documents: 6349, terms: 8654)&gt;&gt; ## Non-/sparse entries: 33788/54910458 ## Sparsity : 100% ## Maximal term length: 24 ## Weighting : term frequency (tf) This is a very sparse (nearly 100% sparse) matrix documents as rows and distinct words as columns. # group_by(chapter) %&gt;% # mutate(text = paste(text, collapse = &quot; &quot;)) %&gt;% # slice_head(n = 1) %&gt;% # select(chapter, text) # # sawyer_sent &lt;- sawyer %&gt;% # sentSplit(&quot;text&quot;) # # skimr::skim(sawyer) quanteda dafdafd tidytext dafdafd 2.2 Subjectivity Lexicons A subjectivity lexicon is a predefined list of words associated with emotional context such as positive/negative. qdap::polarity() uses the lexicon::hash_sentiment_huliu lexicon with sentiment values in (+1, 0, -1.05, -1, -2). It is similar to tidytext::sentiments() AFINN lexicon (-5 to 5). tidytext::sentiments() also includes the NRC lexicon (classifying among 8 emotions) and the Bing lexicon (classifying as positive or negative). Subjectivity lexicons are typically short (a few thousand words), but work because of Zipfs law. According to this law, the nth-ranked item in a frequency table has a frequency count equal to 1/n of the top-ranked item. So infrequently used words are used very infrequently. 2.3 Polarity Scoring 2.3.1 qdap qdap::polarity(text.var, grouping.var = NULL) calculates the polarity score for each character string text.var, grouping by optional character vector grouping.var. polarity uses the sentiment dictionary to tag polarized words. It considers a context cluster of words around polarized words as valence shifters (neutral, negator, amplifier, or de-amplifier). Neutral words hold no value but do affect word count. polarity applies the dictionary weights to each polarized word and then further weights by the number and position of the valence shifters. Last, it sums the context cluster and divides by the square root of the word count, yielding an unbounded polarity score. Lets look at the 35 chapters of Tom Sawyer. sawyer_tm_polarity &lt;- sawyer %&gt;% mutate(text = str_remove_all(text, &quot;\\\\_&quot;)) %$% polarity(text, chapter) ## Warning in polarity(text, chapter): ## Some rows contain double punctuation. Suggested use of `sentSplit` function. scores(sawyer_tm_polarity) ## chapter total.sentences total.words ave.polarity sd.polarity stan.mean.polarity ## 1 1 240 2433 -0.020 0.242 -0.085 ## 2 2 170 1943 0.041 0.237 0.173 ## 3 3 186 2278 -0.013 0.269 -0.047 ## 4 4 302 3487 0.043 0.287 0.150 ## 5 5 155 1947 -0.016 0.280 -0.057 ## 6 6 347 3552 0.012 0.249 0.048 ## 7 7 180 1934 -0.021 0.238 -0.089 ## 8 8 153 1773 -0.048 0.272 -0.176 ## 9 9 201 2167 -0.080 0.279 -0.286 ## 10 10 194 2065 -0.080 0.261 -0.308 ## 11 11 136 1488 -0.040 0.242 -0.163 ## 12 12 147 1702 -0.087 0.294 -0.294 ## 13 13 222 2493 -0.036 0.296 -0.123 ## 14 14 177 2096 -0.025 0.294 -0.084 ## 15 15 140 1697 -0.030 0.265 -0.113 ## 16 16 276 3196 -0.046 0.269 -0.172 ## 17 17 99 1138 0.000 0.225 0.001 ## 18 18 260 2934 0.004 0.250 0.017 ## 19 19 73 791 0.001 0.211 0.007 ## 20 20 145 1700 -0.075 0.296 -0.254 ## 21 21 189 2174 0.039 0.315 0.123 ## 22 22 86 1027 -0.157 0.327 -0.479 ## 23 23 190 1949 -0.058 0.284 -0.203 ## 24 24 35 409 -0.090 0.341 -0.263 ## 25 25 214 2087 -0.009 0.219 -0.041 ## 26 26 250 2631 -0.026 0.274 -0.096 ## 27 27 87 965 0.013 0.252 0.050 ## 28 28 97 1088 0.034 0.226 0.150 ## 29 29 220 2606 0.000 0.263 -0.001 ## 30 30 273 3068 -0.043 0.277 -0.154 ## 31 31 268 3043 -0.056 0.276 -0.203 ## 32 32 87 1030 -0.040 0.278 -0.145 ## 33 33 307 3377 -0.024 0.256 -0.094 ## 34 34 82 879 0.073 0.274 0.266 ## 35 35 161 1919 0.002 0.269 0.006 The counts() function returns one row for each line of text. It includes a list of the positive and negative words that contribute to the polarity score. Line 57 has a polarity score of zero because it has a pair of positive and negative words. sawyer[57,] ## # A tibble: 1 x 3 ## line chapter text ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 57 1 tomorrow, to punish him. It&#39;s mighty hard to make him work Satu~ counts(sawyer_tm_polarity)[57,] ## chapter wc polarity pos.words neg.words text.var ## 57 1 12 0 mighty, work punish, hard tomorrow, to punish him. It&#39;s mighty hard to make him work Saturdays, counts(sawyer_tm_polarity)[57, c(&quot;pos.words&quot;, &quot;neg.words&quot;)] %&gt;% unlist() ## pos.words1 pos.words2 neg.words1 neg.words2 ## &quot;mighty&quot; &quot;work&quot; &quot;punish&quot; &quot;hard&quot; Oh, but wait - Twain doesnt use mighty as a positive adjective, but rather, as an amplifier adverb. Mighty appears 26 times in Tom Sawyer. We should remove it from the polarity.frame and add it to the amplifiers. custom_frame &lt;- sentiment_frame( positives = qdapDictionaries::positive.words[qdapDictionaries::positive.words != &quot;mighty&quot;], negatives = qdapDictionaries::negative.words ) sawyer_tm_polarity_2 &lt;- sawyer %&gt;% mutate(text = str_remove_all(text, &quot;\\\\_&quot;)) %$% polarity( text, chapter, polarity.frame = custom_frame, amplifiers = sort(c(qdapDictionaries::amplification.words, &quot;mighty&quot;)) ) ## Warning in polarity(text, chapter, polarity.frame = custom_frame, amplifiers = sort(c(qdapDictionaries::amplification.words, : ## Some rows contain double punctuation. Suggested use of `sentSplit` function. counts(sawyer_tm_polarity_2)[57,] ## chapter wc polarity pos.words neg.words text.var ## 57 1 12 -0.289 work punish, hard tomorrow, to punish him. It&#39;s mighty hard to make him work Saturdays, Something is still wrong here. It removed mighty as a positive word, but did not apply it as amplifier. It seems to be confused by the presence of the comma in tomorrow, to punish. Ill drop the matter for now, but perhaps how we parse the data into rows makes a difference. It also advises that you run SentSplit() on the data first, but the function never stopped running, so I abandoned it. Here is a plot of the polarity results. plot(sawyer_tm_polarity_2) Chapter 22 had the lowest polarity score and chapter 34 the highest. sawyer_tm_polarity_2 %&gt;% scores() %&gt;% mutate(chapter = as.integer(chapter)) %&gt;% ggplot(aes(x = chapter, y = ave.polarity)) + geom_point() + geom_segment(aes(x = chapter, xend = chapter, y = 0, yend = ave.polarity)) + geom_smooth() + geom_hline(yintercept = 0, color = &quot;red&quot;) + theme_minimal() + labs(title = &quot;Adventures of Tom Sawyer Chronological Polarity&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Create to strings, one with the positive chapters, and one from the negative chapters. sawyer_poloarity_pos &lt;- sawyer_tm_polarity_2$all %&gt;% filter(polarity &gt; 0) %&gt;% pull(text.var) %&gt;% paste(collapse = &quot; &quot;) sawyer_poloarity_neg &lt;- sawyer_tm_polarity_2$all %&gt;% filter(polarity &lt; 0) %&gt;% pull(text.var) %&gt;% paste(collapse = &quot; &quot;) sawyer_polarity_tdm &lt;- c(sawyer_poloarity_pos, sawyer_poloarity_neg) %&gt;% VectorSource() %&gt;% VCorpus() %&gt;% TermDocumentMatrix(control = list(weighting = weightTfIdf, removePunctuation = TRUE, stopwords = stopwords(kind = &quot;en&quot;))) Often authors will use more words when they are more passionate. Lengthy reviews may inflate overall sentiment since the reviews will inherently contain more positive or negative language as the review lengthens. 2.3.2 tidytext The tidy way to score polarity is tagging individual words as positive and negative using the bing lexicon, then defining polarity as difference in counts. sawyer_tidy &lt;- sawyer %&gt;% unnest_tokens(output = &quot;word&quot;, input = text) sawyer_tidy_polarity &lt;- sawyer_tidy %&gt;% inner_join(get_sentiments(&quot;bing&quot;), by = &quot;word&quot;) %&gt;% count(chapter, sentiment) %&gt;% pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% mutate(polarity = positive - negative, polarity_desc = if_else(polarity &gt;= 0, &quot;Positive&quot;, &quot;Negative&quot;)) %&gt;% inner_join(sawyer_tidy %&gt;% count(chapter), by = &quot;chapter&quot;) sawyer_tidy_polarity %&gt;% ggplot(aes(x = chapter, y = polarity)) + geom_point() + geom_segment(aes(x = chapter, xend = chapter, y = 0, yend = polarity)) + geom_smooth() + geom_hline(yintercept = 0, color = &quot;red&quot;) + theme_minimal() + labs(title = &quot;Adventures of Tom Sawyer Chronological Polarity&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; sawyer_tidy_polarity %&gt;% ggplot(aes(x = polarity, y = n, color = polarity_desc)) + geom_point(alpha = 0.6) + geom_smooth(method = &quot;lm&quot;, formula = &quot;y ~ x&quot;, se = FALSE) + scale_color_few() + theme_minimal() + theme(legend.position = &quot;top&quot;) + labs(title = &quot;Polarity is usually a function of length&quot;, color = NULL) In this analysis, the most positive chapter is 4 and the most negative is 9. 2.4 Word Counts If you are using a bag of words approach (ignoring valence-shifters), you can simply count words in the two ranges, positive and negative. sawyer_tidy %&gt;% inner_join(get_sentiments(&quot;bing&quot;), by = &quot;word&quot;) %&gt;% count(word, sentiment) %&gt;% group_by(sentiment) %&gt;% slice_max(order_by = n, n = 8) %&gt;% mutate(n = if_else(sentiment == &quot;negative&quot;, -n, n)) %&gt;% ggplot(aes(x = fct_reorder(word, n), y = n, fill = sentiment, color = sentiment)) + geom_col(alpha = 0.6) + scale_fill_few() + scale_color_few() + coord_flip() + theme_minimal() + theme(legend.position = &quot;top&quot;) + labs(title = &quot;Tom Sawyer Top Sentiment Words&quot;, x = &quot;Word Count&quot;, y = NULL, fill = NULL, color = NULL) 2.5 Comparison Cloud Word clouds are a nice way to get an overview of the data. sawyer_tm_polarity_2$all %&gt;% mutate(polarity = case_when(polarity &lt; 0 ~ &quot;Negative&quot;, polarity &gt; 0 ~ &quot;Positive&quot;, TRUE ~ &quot;Neutral&quot;)) %&gt;% unnest_tokens(output = &quot;word&quot;, input = text.var) %&gt;% anti_join(stop_words, by = &quot;word&quot;) %&gt;% filter(!str_detect(word, &quot;[0-9]&quot;) &amp; polarity != &quot;Neutral&quot;) %&gt;% count(word, polarity, wt = wc) %&gt;% pivot_wider(names_from = polarity, values_from = n, values_fill = 0) %&gt;% data.table::data.table() %&gt;% as.matrix(rownames = &quot;word&quot;) %&gt;% wordcloud::comparison.cloud(max.words = 50) sawyer_tidy %&gt;% anti_join(stop_words, by = &quot;word&quot;) %&gt;% filter(!str_detect(word, &quot;[0-9]&quot;)) %&gt;% inner_join(get_sentiments(&quot;nrc&quot;), by = &quot;word&quot;) %&gt;% filter(!sentiment %in% c(&quot;positive&quot;, &quot;negative&quot;)) %&gt;% count(sentiment, word) %&gt;% pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% data.frame(row.names = &quot;word&quot;) %&gt;% wordcloud::comparison.cloud(max.words = 50, title.size = 1.5) sawyer_tidy %&gt;% anti_join(stop_words, by = &quot;word&quot;) %&gt;% filter(!str_detect(word, &quot;[0-9]&quot;)) %&gt;% inner_join(get_sentiments(&quot;nrc&quot;), by = &quot;word&quot;) %&gt;% filter(sentiment %in% c(&quot;positive&quot;, &quot;negative&quot;)) %&gt;% count(chapter, sentiment) %&gt;% group_by(chapter) %&gt;% mutate(pct = n / sum(n)) %&gt;% ggplot(aes(x = chapter, y = pct, fill = sentiment, color = sentiment)) + geom_area(alpha = 0.6) + scale_x_continuous(breaks = 1:35, minor_breaks = NULL) + scale_fill_few() + scale_color_few() + geom_hline(yintercept = 0.5, linetype = 2) + theme_minimal() + theme(legend.position = &quot;top&quot;) + labs(title = &quot;Sentiment Proportion by Chapter&quot;, x = NULL, y = NULL, fill = NULL, color = NULL) sawyer_tidy %&gt;% anti_join(stop_words, by = &quot;word&quot;) %&gt;% filter(!str_detect(word, &quot;[0-9]&quot;)) %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;% ggplot(aes(x = value)) + geom_density(fill = ggthemes::few_pal()(1), alpha = 0.6) + theme_minimal() + labs(title = &quot;AFINN Score Density&quot;) sawyer_tidy %&gt;% inner_join(get_sentiments(&quot;bing&quot;), by = &quot;word&quot;) %&gt;% count(chapter, line, sentiment) %&gt;% pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% mutate(polarity = positive - negative) %&gt;% ggplot(aes(x = as.factor(chapter), y = polarity)) + geom_boxplot() + geom_jitter(aes(color = as.factor(chapter)), alpha = 0.6, size = .5, show.legend = FALSE) + theme_minimal() + labs(title = &quot;Chapter Polarity&quot;) Bar plots are usually a clearer alternative, but radar charts do look pretty. https://en.wikipedia.org/wiki/Robert_Plutchik#/media/File:Plutchik-wheel.svg dat &lt;- sawyer_tidy %&gt;% inner_join(get_sentiments(&quot;nrc&quot;), by = &quot;word&quot;) %&gt;% filter(!sentiment %in% c(&quot;positive&quot;, &quot;negative&quot;)) %&gt;% mutate(sentiment = case_when(sentiment == &quot;joy&quot; ~ 1, sentiment == &quot;trust&quot; ~ 2, sentiment == &quot;fear&quot; ~ 3, sentiment == &quot;surprise&quot; ~ 4, sentiment == &quot;sadness&quot; ~ 5, sentiment == &quot;disgust&quot; ~ 6, sentiment == &quot;anger&quot; ~ 7, sentiment == &quot;anticipation&quot; ~ 8, TRUE ~ 9), sentiment = factor(sentiment, levels = c(1:9), labels = c(&quot;joy&quot;, &quot;trust&quot;, &quot;fear&quot;, &quot;surprise&quot;, &quot;sadness&quot;, &quot;disgust&quot;, &quot;anger&quot;, &quot;anticipation&quot;, &quot;other&quot;))) %&gt;% count(sentiment) dat %&gt;% radarchart::chartJSRadar() sawyer_tidy %&gt;% inner_join(get_sentiments(&quot;afinn&quot;), by = &quot;word&quot;) %&gt;% count(chapter, value) %&gt;% ggplot(aes(area = n, fill = value)) + treemapify::geom_treemap() "]]
