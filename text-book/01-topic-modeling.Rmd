# Topic Modeling {#topicmodeling}

Topic models are unsupervised ML models that identify topics as clusters of words with an associated probability distribution, and a probability distribution of topics within each document. 

## LDA {#lda}

Latent Dirichlet allocation (LDA) is an instance of a general family of mixed membership models that decompose data into latent components. *Latent* refers to unidentified topics. *Dirichlet* refers to the type of distribution followed by the words in the the topics and by the topics in the documents.

#### Algorithm {-}

LDA assumes each document is created by a generative process where topics are included according to probabilities and words are included in the topics according to probabilities. The LDA algorithm determines what those probabilities are. The [algorithm](https://www.mygreatlearning.com/blog/understanding-latent-dirichlet-allocation/) is:

1) For each document $d_i$, randomly assign each word to one of the *K* topics. Note that each $w_j$ may be assigned to a different topic in each documents.

2) For each document, tabulate the number of words in each topic, a $d \times K$ matrix. For each word, tabulate the sum of occurrences across all documents, a $w \times K$ matrix.

3) Resample a single instance of a word from the corpus and remove it from the analysis, decrementing the document's topic count and the word's topic count.

4) Calculate the gamma matrix, $\gamma$, and the beta matrix, $\beta$.
    * the gamma matrix is the probability distribution of topics for each document, $$p(t_k|d_i) = \frac{n_{ik} + \alpha}{N_i + K \alpha}$$ were $n_{ik}$ is the number of words in document $i$ for topic $k$, $N_i$ is the total number of words in $i$, and $\alpha$ is a hyperparameter. For each $d_i$, $\sum_{k \in K} \gamma_{ik} = 1$.
    * the beta matrix is the probability distribution of words for each topic, $$p(w_j|t_k) = \frac{m_{j,k} + \beta}{\sum_{j \in V}m_{j,k} + V\beta}$$ where $m_{j,k}$ is the corpus-wide frequency count of word $w_j$ to topic $k$, $V$ is the number of distinct words in the corpus, and $\beta$ is a hyperparameter. For each $t_k$, $\sum_{j \in V} \beta_{kj} = 1$.
  
6) Perform Gibbs sampling. Calculate the joint probability distribution of words for each document and topic, $p(w_j|t_k,d_i) = p(t_k|d_i)p(w_j|t_k)$. Assign each word, $w_j$, to the topic with the maximum joint probability.

7) Repeat steps 3-6 for all of the words in all of the documents.

8) Repeat steps 3-7 for a pre-determined number of iterations.

LDA thus has 3 hyperparameters: document-topic density factor, $\alpha$, topic-word density factor, $\beta$, and topic count, $K$. $\alpha$ controls the number of topics expected per document (large $\alpha$ = more topics). $\beta$ controls the distribution of words per topic (large $\beta$ = more words). Ideally, you want a few topics per document and a few words per topics, so, $\alpha$ and $\beta$ are typically set below one. $K$ is set using a combination of domain knowledge, *coherence*, and exclusivity.

#### Evaluation {-}

##### Held-out Likelihood {-}

(discussion of hold-out probability) (Wallach et al., 2009).

##### Semantic Coherence {-}

##### Exclusivity {-}

Generally, the greater the number of topics in a model, the lower the quality of the smallest topics. One way around this is simply hiding the low-quality topics. The coherence measure [@10.5555/2145432.2145462] evaluates topics.


## CTM {#ctm}

The Correlated Topic Model (CTM) [@blei2007] builds on the LDA model (chapter \@ref(lda)). 

## STM

Without the inclusion of covariates, STM reduces to a logistic-normal topic model, often
called the Correlated Topic Model (CTM) (chapter \@ref(ctm)).
