---
title: "An Analytic Approach to The Theory of Moral Sentiments"
subtitle: "A structural topic model of the text"
author: "Michael Foley"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: flatly
    toc: true
    toc_float: true
    highlight: haddock
    fig_height: 3
    fig_width: 5
    fig_caption: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This project uses structural topic modeling to explore themes in Adam Smith's <u>The Theory of Moral Sentiments</u>. Treating each of the 55 chapters of the text as independent documents, overlapping themes emerge that tie the parts together.

## Introduction

Students of Adam Smith's <u>The Theory of Moral Sentiments</u> (TMS) discover its major themes through a holistic reading of the text, detailed parsing of individual sections, and placement in social context. An alternative approach made possible through machine learning algorithms is to treat the text as a "bag of words" in which repeated usage within and across sub-sections identifies themes that unite the overall book.

In the analysis that follows, I treat the 55 chapters of TMS as "documents" representing thoughts, ideas, and illustrations of the TMS "corpus". I employ a structural topic model (STM) to identify the topics.

### A brief overview of accepted themes

I found a nice synopsis of seven major themes on [GradeSaver](https://www.gradesaver.com/the-theory-of-moral-sentiments/study-guide/themes):

* **Sympathy**. People judge the actions of others by imagining how they would feel if placed in the other's situation. This stepping into different perspectives is *sympathy*.
* **Appearances**. People are more interested in their own affairs over others'. As a result, others are more likely to sympathize with displays of emotion that are tempered to their own level of interest. The restraint of one's passions constitutes the virtue of *temperance*.
* **Conscience**. One's conscience is an inner moral guide. It evaluates actions from the perspective of a third party observer. Through our conscience we are imagine our behavior will be judged by others before ever acting.
* **Justice**. While other virtues are laudable, justice is what is *owed* to ourselves and to each other. It is therefore not worthy of praise, only of the absence of disdain.
* **Harmony within Systems**. Smith celebrates how the harmony of the system exemplifies the principles of intelligent design.
* **The Difference in Magnitude between Positive and Negative Sentiments**. Risky situations usually present the actor with more to lose than to gain. Therefore, people are unlikely to take risks unless there is an expectation of great reward. Conferring glory is one way society furnishes reward. Similarly, sympathy for loss tends to be greater than sympathy for unachieved gain.
* **The Corruption of Moral Sentiments by Wealth**. Respect for wealth rewards pro-social behavior generally. However, the equivalence of wealth with virtue is a moral corruption.

### Structural Topic Modeling

STM is a generative model, meaning the researcher defines a data generating process for each document (TMS chapter for this project) then uses the document text to find the most likely values for the model parameters. The generative model defines document-topic distributions, and word-topic distributions generating documents. The topic proportions (topical prevalences) in each document sum to one, and word probabilities in each topic (topical content) sum to one.

The generative process for each document is a follows...

## Method

### Data

```{r message=FALSE, warning=FALSE, echo=FALSE}
library(tidyverse)
library(tidytext)
library(ggthemes)
#library(textstem)     # lemmatize words (not used)
library(stm)          # structure topic modeling
library(furrr)        # future_map
```

The complete text of TMS is available from the [Liberty Fund](https://oll.libertyfund.org/titles/smith-the-theory-of-moral-sentiments-and-on-the-origins-of-languages-stewart-ed/simple). The text includes an introduction by Dugald Stewart, and an appendix on the origin of languages, both of which I removed. I also parsed out the part, section, and chapter identifiers as metadata for the model. TMS consists of seven parts with 1-4 sections per part and a total of 55 chapters.

```{r message=FALSE, warning=FALSE}
library(rvest)

tms_url <- "https://oll.libertyfund.org/titles/smith-the-theory-of-moral-sentiments-and-on-the-origins-of-languages-stewart-ed/simple"

tms_split <- read_html(tms_url) %>%
  html_text() %>%
  str_split(pattern = "\\n") %>%
  unlist() %>% 
  tail(-600) %>% 
  head(-178)

tms_paragraphs <- data.frame(Text = tms_split) %>%
  mutate(
    Text = str_remove_all(Text, "Edition: current; Page: \\[[:digit:]+\\] "),
    Part = str_extract(Text, regex("^Part\\s(First|Second|Third|Fourth|Fifth|Sixth|Seventh)")),
    Section = str_extract(Text, regex("^SECTION\\s(IV|III|II|I)")),
    Section = if_else(Part %in% c("Part Third", "Part Fourth", "Part Fifth"), "SECTION I", Section),
    Chapter = str_extract(Text, regex("^((CHAPTER\\s(VI|V|IV|III|II|I))|INTRODUCTION|CONCLUSION)"))
  ) %>%
  fill(Part, Section, Chapter, .direction = "down") %>%
  mutate(
    Chapter = if_else(Part == "Part Sixth" & Section == "SECTION I", "N/A", Chapter),
    Chapter = if_else(Part == "Part Sixth" & Section == "SECTION III" & Chapter == "CHAPTER III", "N/A", Chapter),
    Chapter = if_else(Part == "Part Seventh" & Section == "SECTION I", "N/A", Chapter),
    Chapter = if_else(Part == "Part Seventh" & Section == "SECTION IV", "N/A", Chapter)
  ) %>%
  filter(!str_detect(Text, regex("^Part\\s(First|Second|Third|Fourth|Fifth|Sixth|Seventh)")) &
           !str_detect(Text, regex("^SECTION\\s(IV|III|II|I)")) &
           !str_detect(Text, regex("^((CHAPTER\\s(VI|V|IV|III|II|I))|INTRODUCTION|CONCLUSION)")) &
           !str_detect(Text, regex("^Edition:\\scurrent;\\sPage:")) &
           Text != "") %>%
  mutate(
    Part = fct_inorder(Part),
    Section = fct_inorder(Section),
    Chapter = fct_inorder(Chapter),
    Chapter = fct_relevel(Chapter, "INTRODUCTION", "N/A", after = 0),
    Chapter = fct_relevel(Chapter, "CONCLUSION", after = Inf),
    id = row_number()
  ) %>%
  group_by(Part, Section, Chapter) %>%
  mutate(one = 1, 
         Paragraph = cumsum(one)) %>%
  ungroup() %>%
  select(id, Part, Section, Chapter, Paragraph, Text)

tms_chapters <- tms_paragraphs %>%
  arrange(Paragraph) %>%
  group_by(Part, Section, Chapter) %>%
  mutate(Paragraphs = n(), 
         Chapter_Text = str_flatten(Text)) %>%
  ungroup() %>%
  distinct(Part, Section, Chapter, Paragraphs, Text = Chapter_Text) %>%
  mutate(id = row_number()) %>%
  select(id, Part, Section, Chapter, Paragraphs, Text)

tms_chapters %>% 
  mutate(Text = paste0(str_sub(Text, 1, 200), "...")) %>%
  select(-id) %>%
  head(6) %>%
  flextable::flextable() %>%
  flextable::autofit() %>%
  flextable::set_caption("TMS data parsed for analysis - first six records.")
```

### Preparation

The first step is to convert the text into a bag-of-words representation by tokenizing the chapters into individual words. Each word of TMS becomes a separate row of data described by its location in the book (part, section, chapter). There are 150,905 words in TMS (8,155 distinct). From this arrangement I removed "stop words" such as *a*, *and*, and *across* that occur frequently but are irrelevant to topics. The footnote number identifiers also appear as tokens, so I removed them too. That reduces the word count to 49,901 (7,270 distinct). Some analyses also [lemmatize](https://en.wikipedia.org/wiki/Lemmatisation) the tokens, but [that is not always helpful](http://www.cs.cornell.edu/~xanda/winlp2017.pdf) so I left the words intact. Finally, I removed the very infrequently appearing words that will drag down computation time without contributing to theme formation. I set the lower frequency limit to 10 appearances in TMS. This brings the word count down to 34,941 words (1,090 distinct). You can see from the first few records how the first line of text, "How selfish soever man may be supposed, there are evidently some principles in his nature", tokenizes to just six words.

```{r}
tms_tokens <- tms_chapters %>%
  unnest_tokens(output = "word", input = Text, token = "words") %>%
  anti_join(stop_words, by = "word") %>%
  filter(!str_detect(word, "[0-9]+")) %>%
  add_count(word) %>%
  filter(n >= 10) %>%
  select(-n)

tms_tokens %>% 
  select(`Part ` = Part, everything(), -id, -Paragraphs) %>%
  head(6) %>%
  flextable::flextable() %>%
  flextable::autofit() %>%
  flextable::set_caption("TMS data tokenized - first line of text.")
```

At this point the words can reveal some of their relative importance through their TF-IDF statistic. A term's frequency (TF) is its proportion of the terms in the document. The inverse document frequency (IDF) is the log of the inverse ratio of documents in which the term appears. The product of TF and IDF (TF-IDF) indicates how important a term is to a document within the corpus. Below I treated the parts as the documents within the TMS corpus. For each part, each word's TF-IDF increases with its frequency within the part and decreases with the number of parts in which it appears.

```{r fig.width=8, fig.height=10}
tms_tfidf <- tms_tokens %>%
  count(id, word, sort = TRUE) %>%
  bind_tf_idf(word, id, n) %>%
  inner_join(tms_chapters %>% select(-Text), by = "id")

tms_tfidf %>%
  group_by(Part, word) %>%
  summarize(.groups = "drop", n = sum(n), tf = mean(tf), idf = mean(idf), tf_idf = mean(tf_idf)) %>%
  ggplot(aes(x = tf, y = idf)) +
  geom_point(aes(color = tf_idf, size = tf_idf), alpha = 0.6, show.legend = FALSE) +
  geom_text(aes(label = word, size = tf_idf), 
            color = "#666666", check_overlap = TRUE, show.legend = FALSE) +
  scale_colour_gradient(low = "#cccccc", high = "#116530") +
  theme_minimal() +
  labs(title = "Average TF-IDF by Part of TMS",
       subtitle = "Larger := more 'important'.") +
  facet_wrap(~Part, ncol = 2)
```

The word "custom" appears 47 times in Part Fifth of TMS - 23 times in Chapter I and 24 times in Chapter 2 - but only twice more in the rest of TMS (once in Part Second and once in Part Seventh). That gives "custom" a particularly high IDF for both chapters of Part Fifth. Within the Part Fifth chapters, "custom" appeared frequently - 4% of the words in chapter I and 2% of the words in chapter II. That gives "custom" a high TF in those chapters as well. Their products, TF-IDF, were therefore large, and their chapter-average in Part Fifth, makes "custom" an important word to Part Fifth.

Next I pivoted the data into a *sparse matrix* with one row per chapter (55 rows), one column per word (1,090 cols), with cell values equal to the word's frequency count. 

```{r results='hide'}
tms_cnt <- tms_tokens %>% count(id, word)

tms_sparse <- tms_cnt %>% cast_sparse(id, word, n)

tms_cnt %>%
  ggplot(aes(x = word, y = id, fill = n)) +
  geom_tile(show.legend = FALSE, alpha = 0.6) +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  labs(y = "Chapter",
       title = "TMS Sparse Matrix",
       subtitle = "Heat map of word frequencies.")
```

The matrix is `r scales::percent(sum(tms_sparse %>% as.matrix() == 0) / (55*1090), accuracy = 1)` zeros. Now the data is ready for modeling.

### Estimation

There is no "right" number of topics in a corpus, but there are diagnostic measures that can aid human
judgment in the creation of well-performing model. By fitting models of varying numbers of topics, the diagnostic measures can be compared. 

One diagnostic measure is the held-out likelihood. In this measure, the model is re-fit with random words removed from documents. Then the model estimates the probabilities that those words are in the documents. A better model will have higher probabilities on the held-out words. The highest held-out likelihood occurred at *K* = 20 topics. I don't understand how the likelihood is measured. [This article](https://mimno.infosci.cornell.edu/info6150/readings/wallach09evaluation.pdf), as well as Section 8.1 of the [STM vignette](https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf) may help.

A second diagnostic measure is the residuals. If residuals have a high variance, more topics may be needed.

The lower bound is the approximation to the lower bound on the marginal likelihood. You can think of it as the model's internal measure of fit. Convergence is monitored by the change in the approximate
variational lower bound. Once the bound has a small enough change between iterations, the
model is considered converged.

We then examined the set of estimated models that fit the data particularly well and compared
them using their scores on average semantic coherence and exclusivity. (Coherence measures the internal consistency of the topics
and exclusivity captures the extent to which topics in the model can be differentiated from each other.) Our initial goal was to
identify the set of models that were not strictly dominated by other models in terms of semantic coherence and exclusivity. Then,
using our own judgment, we examined the cohesiveness and exclusivity of the topics in the set of non-dominated models, that is,
those models located on the 'semantic coherence-exclusivity frontier' (Roberts et al., 2014). This process resulted in choosing 16 for
the number of topics. We verified that all of our substantive findings were robust to small variations in the number of estimated
topics.

```{r}
#  tms_proc <- textProcessor(
#   documents = tms_df$Text, 
#   metadata = tms_df %>% select(-id, -Text),
#   removestopwords = TRUE,
#   removenumbers = TRUE,
#   removepunctuation = TRUE,
#   stem = FALSE
# )
# 
# tms_prep <- prepDocuments(tms_proc$documents, tms_proc$vocab, tms_proc$meta)
# 
# tms_stm <- stm(
#   documents = tms_prep$documents, 
#   vocab = tms_prep$vocab, 
#   K = 0,  # let algorithm select K
#   prevalence =~ Part + Section + Chapter,
#   data = tms_prep$meta,
#   init.type = "Spectral"
# )
```

```{r}
plan(multiprocess)

stm_mdls <- data.frame(K = c(4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24)) %>%
  mutate(mdl = future_map(K, ~stm(tms_sparse, K = ., verbose = FALSE),
                          .options = furrr_options(seed = 123)))
```

```{r}
heldout <- make.heldout(tms_sparse)

k_result <- stm_mdls %>%
  mutate(exclusivity = map(mdl, exclusivity),
         semantic_coherence = map(mdl, semanticCoherence, tms_sparse),
         eval_heldout = map(mdl, eval.heldout, heldout$missing),
         residual = map(mdl, checkResiduals, tms_sparse),
         bound =  map_dbl(mdl, function(x) max(x$convergence$bound)),
         lfact = map_dbl(mdl, function(x) lfactorial(x$settings$dim$K)),
         lbound = bound + lfact,
         iterations = map_dbl(mdl, function(x) length(x$convergence$bound)))

k_result %>%
  transmute(K,
            `Lower bound` = lbound,
            Residuals = map_dbl(residual, "dispersion"),
            `Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "K (number of topics)",
       y = NULL,
       title = "Model diagnostics by number of topics",
       subtitle = "These diagnostics indicate that a good number of topics would be around 18")
```

```{r}
k_result %>%
  select(K, exclusivity, semantic_coherence) %>%
  filter(K %in% c(5, 10, 15)) %>%
  unnest(-K) %>%
  mutate(K = as.factor(K)) %>%
  ggplot(aes(semantic_coherence, exclusivity, color = K)) +
  geom_point(size = 2, alpha = 0.7) +
  labs(x = "Semantic coherence",
       y = "Exclusivity",
       title = "Comparing exclusivity and semantic coherence",
       subtitle = "Models with fewer topics have higher semantic coherence for more topics, but lower exclusivity")
```


```{r}
# tms_stm <- stm(
#   tms_sparse, 
#   K = 0, 
# #  prevalence =~ Part,
#   data = tms_df,
#   init.type = "Spectral", 
#   verbose = FALSE
# )
# tms_stm
```


## Results
descriptive stats, analysis

## Coinclusions an dlimitations.






## The STM Model

Topical prevalence captures how much each topic contributes to a document. Because different documents come from different sources, it is natural to want to allow this prevalence to vary with metadata that we have about document sources. We will let prevalence be a function of the “rating” variable, which is coded as either “Liberal” or “Conservative,” and the variable “day.” which is an integer measure of days running from the first to the last day of 2008. 

[Toward understanding 17th century English culture: A structural
topic model of Francis Bacon's ideas](https://www.econ.umd.edu/sites/www.econ.umd.edu/files/users/pmurrell/Bacon_Grajzl-MurrellJCE2019.pdf)

